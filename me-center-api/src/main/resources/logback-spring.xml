<?xml version="1.0" encoding="UTF-8"?>
<!-- 从高到地低 OFF 、 FATAL 、 ERROR 、 WARN 、 INFO 、 DEBUG 、 TRACE 、 ALL -->
<!-- 日志输出规则 根据当前ROOT 级别，日志输出时，级别高于root默认的级别时 会输出 -->
<!-- 以下 每个配置的 filter 是过滤掉输出文件里面，会出现高级别文件，依然出现低级别的日志信息，通过filter 过滤只记录本级别的日志 -->
<!-- 属性描述 scan：性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。
	debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 -->
<configuration scan="true" scanPeriod="60 seconds" debug="false">
    <!-- 定义日志文件 输入位置 -->
    <property name="log_dir" value="/data/webroot/workorder-csc-api/logs" />
    <!-- 日志最大的历史 15天 -->
    <property name="maxHistory" value="30" />
    <property name="pattern" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] [%-5level] %logger{30}.%method:%line [%X{reqKey}] - %msg%n" />

    <springProperty scope="context" name="sentryEnable" source="sentry.enable" defaultValue="false" />

    <springProperty scope="context" name="elkEnable" source="elk.enable" defaultValue="false" />
    <springProperty scope="context" name="elkHost" source="elk.host" defaultValue="localhost" />
    <springProperty scope="context" name="elkPort" source="elk.port" defaultValue="5672" />
    <springProperty scope="context" name="elkUsername" source="elk.username" defaultValue="test" />
    <springProperty scope="context" name="elkPassword" source="elk.password" defaultValue="itsme999" />
    <springProperty scope="context" name="elkVirtualHost" source="elk.virtualHost" defaultValue="/dev" />
    <springProperty scope="context" name="elkExchange" source="elk.exchange" defaultValue="logback-exchange" />
    <springProperty scope="context" name="elkQueue" source="elk.queue" defaultValue="logback-queue" />
    <springProperty scope="context" name="elkRoutingKey" source="elk.routingKey" defaultValue="" />
    <springProperty scope="context" name="elkIdentifier" source="elk.identifier" defaultValue="UNDEFINED" />
    <springProperty scope="context" name="elkClientProvidedName" source="elk.clientProvidedName" defaultValue="UNDEFINED" />

    <!-- ConsoleAppender 控制台输出日志 -->
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <!-- 对日志进行格式化 -->
        <encoder>
            <pattern>${pattern}</pattern>
        </encoder>
    </appender>

    <appender name="ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤器，只记录ERROR级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <file>${log_dir}/error.log</file>
        <!-- 每日或者单个文件超过300M是生成一个新的日志文件，当日志文件超过50个时删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/error_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>300MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="INFO" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 记录info级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <file>${log_dir}/info.log</file>
        <!-- 每日或者单个文件超过300M是生成一个新的日志文件，当日志文件超过50个时删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/info_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>300MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="ASYNC_INFO" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>1024</queueSize>
        <includeCallerData>true</includeCallerData>
        <neverBlock>true</neverBlock>
        <appender-ref ref="INFO" />
    </appender>

    <if condition='property("sentryEnable").contains("true")'>
        <then>
            <appender name="Sentry" class="com.getsentry.raven.logback.SentryAppender">
                <!-- 过滤器，只记录ERROR级别以上的日志 -->
                <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
                    <level>ERROR</level>
                </filter>
                <!--<filter class="com.tuhu.finance.utils.common.filter.SentrySwitchFilter" />-->
                <dsn>http://5e9bcb04a712490c9f5ca40f58305f03:589ad3eb18594bd6af21ff20f6c90271@netscaler.ad.tuhu.cn:9000/147</dsn>
            </appender>
        </then>
    </if>

    <if condition='property("elkEnable").contains("true")'>
        <then>
            <appender name="ELK"
                      class="com.tuhu.finance.logback.appender.rabbitmq.RabbitMQAppender">
                <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
                    <level>INFO</level>
                </filter>
                <host>${elkHost}</host>
                <port>${elkPort}</port>
                <username>${elkUsername}</username>
                <password>${elkPassword}</password>
                <virtualHost>${elkVirtualHost}</virtualHost>
                <exchange>${elkExchange}</exchange>
                <queue>${elkQueue}</queue>
                <routingKey>${elkRoutingKey}</routingKey>
                <type>topic</type>
                <exchangeDurable>true</exchangeDurable>
                <queueDurable>true</queueDurable>
                <identifier>${elkIdentifier}</identifier>
                <clientProvidedName>${elkClientProvidedName}</clientProvidedName>
                <layout class="com.tuhu.finance.logback.appender.rabbitmq.JSONLayout" />
            </appender>
        </then>
    </if>

    <appender name="EVENT" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 记录info级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>INFO</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
        <file>${log_dir}/event.log</file>
        <!-- 每日或者单个文件超过300M是生成一个新的日志文件，当日志文件超过50个时删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/event_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>300MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="ASYNC_EVENT" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>1024</queueSize>
        <includeCallerData>true</includeCallerData>
        <neverBlock>true</neverBlock>
        <appender-ref ref="EVENT" />
    </appender>

    <appender name="EVENT-ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤器，只记录ERROR级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>WARN</level>
        </filter>
        <file>${log_dir}/event-error.log</file>
        <!-- 每日或者单个文件超过300M是生成一个新的日志文件，当日志文件超过50个时删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/event-error_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>300MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="FAILED-EVENT-APPENDER" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤器，只记录ERROR级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <file>${log_dir}/failed-event.log</file>
        <!-- 每日或者单个文件超过300M是生成一个新的日志文件，当日志文件超过50个时删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/failed-event_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>300MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="MONITOR-APPENDER" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 过滤器，只记录ERROR级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <file>${log_dir}/monitor-appender.log</file>
        <!-- 每日或者单个文件超过50M是生成一个新的日志文件，当日志文件超过12个删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/monitor-appender_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>12</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>50MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="DRUID-APPENDER" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <file>${log_dir}/druid-appender.log</file>
        <!-- 每日或者单个文件超过50M是生成一个新的日志文件，当日志文件超过12个删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/druid-appender_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>12</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>50MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="DEFAULT" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 记录info级别以上的日志 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <file>${log_dir}/default.log</file>
        <!-- 每日或者单个文件超过300M是生成一个新的日志文件，当日志文件超过50个时删除更早的文件 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log_dir}/default_%d{yyyy-MM-dd}_%i.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>300MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>${pattern}</pattern>
            <immediateFlush>true</immediateFlush>
        </encoder>
    </appender>

    <appender name="ASYNC_DEFAULT" class="ch.qos.logback.classic.AsyncAppender">
        <discardingThreshold>0</discardingThreshold>
        <queueSize>1024</queueSize>
        <includeCallerData>true</includeCallerData>
        <neverBlock>true</neverBlock>
        <appender-ref ref="DEFAULT" />
    </appender>

    <logger name="com" level="INFO" additivity="false">
        <appender-ref ref="ASYNC_INFO" />
        <appender-ref ref="ERROR" />
        <if condition='property("sentryEnable").contains("true")'>
            <then>
                <appender-ref ref="Sentry" />
            </then>
        </if>
        <if condition='property("elkEnable").contains("true")'>
            <then>
                <appender-ref ref="ELK" />
            </then>
        </if>
        <appender-ref ref="STDOUT" />
    </logger>

    <logger name="com.alibaba.druid" level="INFO" additivity="false">
        <appender-ref ref="DRUID-APPENDER" />
        <if condition='property("sentryEnable").contains("true")'>
            <then>
                <appender-ref ref="Sentry" />
            </then>
        </if>
        <if condition='property("elkEnable").contains("true")'>
            <then>
                <appender-ref ref="ELK" />
            </then>
        </if>
    </logger>

    <logger name="org" level="INFO" additivity="false">
        <appender-ref ref="ASYNC_INFO" />
        <appender-ref ref="ERROR" />
        <if condition='property("sentryEnable").contains("true")'>
            <then>
                <appender-ref ref="Sentry" />
            </then>
        </if>
        <if condition='property("elkEnable").contains("true")'>
            <then>
                <appender-ref ref="ELK" />
            </then>
        </if>
        <appender-ref ref="STDOUT" />
    </logger>

    <logger name="MONITOR-LOGGER" level="ERROR" additivity="false">
        <appender-ref ref="MONITOR-APPENDER" />
        <if condition='property("sentryEnable").contains("true")'>
            <then>
                <appender-ref ref="Sentry" />
            </then>
        </if>
        <if condition='property("elkEnable").contains("true")'>
            <then>
                <appender-ref ref="ELK" />
            </then>
        </if>
    </logger>

    <logger name="org.apache.ibatis" level="debug">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="ASYNC_DEFAULT" />
    </logger>

    <!-- root级别 DEBUG -->
    <root level="DEBUG">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="ASYNC_DEFAULT" />
        <if condition='property("sentryEnable").contains("true")'>
            <then>
                <appender-ref ref="Sentry" />
            </then>
        </if>
        <if condition='property("elkEnable").contains("true")'>
            <then>
                <appender-ref ref="ELK" />
            </then>
        </if>
    </root>
</configuration>